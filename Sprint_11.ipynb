{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sure Tomorrow Insurance Modeling: Project Description\n",
    "\n",
    "The Sure Tomorrow insurance company wants to solve several tasks with the help of Machine Learning and you are asked to evaluate that possibility. After loading, cleaning and performing basic exploratory data analysis, the following four tasks will be completed.\n",
    "\n",
    "- Task 1: Find customers who are similar to a given customer. This will help the company's agents with marketing.\n",
    "- Task 2: Predict whether a new customer is likely to receive an insurance benefit. Can a prediction model do better than a dummy model?\n",
    "- Task 3: Predict the number of insurance benefits a new customer is likely to receive using a linear regression model.\n",
    "- Task 4: Protect clients' personal data without breaking the model from the previous task. It's necessary to develop a data transformation algorithm that would make it hard to recover personal information if the data fell into the wrong hands. This is called data masking, or data obfuscation. But the data should be protected in such a way that the quality of machine learning models doesn't suffer. You don't need to pick the best model, just prove that the algorithm works correctly.\n",
    "\n",
    "Overall conclusions will be summarized at the end of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load Data\n",
    "\n",
    "Load data and conduct a basic check that it's free from obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Projects/TT_S11/insurance_us.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to make the code more consistent\n",
    "df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sample\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info for data types\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change age type from float to int\n",
    "df['age'] = df['age'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm conversion was successful\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check descriptive statistics for anomalies\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details about insurance benefits variable given descriptive stats for that variable\n",
    "\n",
    "df.groupby('insurance_benefits').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Conclusions** The variable ranges look appropriate based on the spread of data for each. For family members, the minimum is 0 and the median 1, indicating that the variable is additional family members other than the primary insured individual.\n",
    "\n",
    "The counts for the insurance benefits variable values was run given the descriptive statistics were limited in what they showed. By running the counts, it is clear that only about 10% of members receive benefits, which may have implications for analysis. For the time being, the counts confirmed the data looks appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Exploratory Data Analysis\n",
    "Check whether there are certain groups of customers by looking at the pair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df, kind='hist')\n",
    "g.fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to spot obvious clusters with several multivariate distributions. Next step is to use other techniques to find similar customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Similar Customers\n",
    "\n",
    "Write a function that returns k nearest neighbors for an $n^{th}$ object based on a specified distance metric. The number of received insurance benefits should not be taken into account for this task. \n",
    "\n",
    "Test it for four combination of two cases\n",
    "- Scaling\n",
    "  - the data is not scaled\n",
    "  - the data is scaled with the [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) scaler\n",
    "- Distance Metrics\n",
    "  - Euclidean\n",
    "  - Manhattan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of feature names to use below\n",
    "\n",
    "feature_names = ['gender', 'age', 'income', 'family_members']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that returns k nearest neighbors\n",
    "\n",
    "def get_knn(df, n, k, metric):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns k nearest neighbors\n",
    "\n",
    "    :param df: pandas DataFrame used to find similar objects within\n",
    "    :param n: object no for which the nearest neighbours are looked for\n",
    "    :param k: the number of the nearest neighbours to return\n",
    "    :param metric: name of distance metric\n",
    "    \"\"\"\n",
    "\n",
    "    nbrs = sklearn.neighbors.NearestNeighbors(metric=metric)\n",
    "    nbrs.fit(df[feature_names])\n",
    "    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)\n",
    "    \n",
    "    df_res = pd.concat([\n",
    "        df.iloc[nbrs_indices[0]], \n",
    "        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])\n",
    "        ], axis=1)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify similar records for a given one for every combination\n",
    "\n",
    "# original data with Manhattan metric \n",
    "get_knn(df, 50, 6, distance.cityblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data with Euclidean metric \n",
    "get_knn(df, 50, 6, distance.euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data with Manhattan metric \n",
    "get_knn(df_scaled, 50, 6, distance.cityblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data with Euclidean metric \n",
    "get_knn(df_scaled, 50, 6, distance.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Conclusions**\n",
    "\n",
    "**Does the data being not scaled affect the kNN algorithm? If so, how does that appear?** \n",
    "\n",
    "Scaling the data pulls different observations as most similar to the test object as compared to unscaled data. When running to identify the five nearest neighbors, only one observation was the same in the scaled and unscaled data.\n",
    "\n",
    "**How similar are the results using the Manhattan distance metric (regardless of the scaling)?** \n",
    "\n",
    "Using Manhattan and Euclidean distance metrics gives very similar results. The observations identified as the nearest neighbors were nearly identical for both metrics when running for the five nearest neighbors. The distances were slightly different but not by large degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Is a Customer Likely to Receive an Insurance Benefit?\n",
    "\n",
    "Evaluate whether the kNN classification approach performs better than a dummy model for determining whether a customer is likely to receive an insurance benefit, i.e., insurance_benefits variable is greater than zero as the target. This is a binary classification task.\n",
    "\n",
    "For this task:\n",
    "- Build a KNN-based classifier and measure its quality with the F1 metric for k=1..10 for both the original data and the scaled one. That'd be interesting to see how k may influece the evaluation metric, and whether scaling the data makes any difference. \n",
    "- Build the dummy model which is just random for this case. It should return \"1\" with some probability. Let's test the model with four probability values: 0, the probability of paying any insurance benefit, 0.5, 1.\n",
    "\n",
    "The probability of paying any insurance benefit can be defined as\n",
    "\n",
    "$$\n",
    "P\\{\\text{insurance benefit received}\\}=\\frac{\\text{number of clients received any insurance benefit}}{\\text{total number of clients}}.\n",
    "$$\n",
    "\n",
    "Split the whole data in the 70:30 proportion for the training/testing parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the target\n",
    "\n",
    "df['insurance_benefits_received'] = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df['insurance_benefits'][i] == 0:\n",
    "        df['insurance_benefits_received'][i] = 0\n",
    "    else:\n",
    "        df['insurance_benefits_received'][i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check target calculated accurately\n",
    "print(df.sample(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaled data set\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())\n",
    "\n",
    "df_scaled_bin = df.copy()\n",
    "df_scaled_bin.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to confirm data was scaled\n",
    "print(df_scaled_bin.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target data for the main df\n",
    "features = df[feature_names]\n",
    "target = df['insurance_benefits_received']\n",
    "\n",
    "print(features.head())\n",
    "print()\n",
    "print(target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and target data for scaled data\n",
    "features_sc = df_scaled_bin[feature_names]\n",
    "target_sc = df_scaled_bin['insurance_benefits_received']\n",
    "\n",
    "print(features_sc.head())\n",
    "print()\n",
    "print(target_sc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data subsets for main df\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features,\n",
    "                                                                   target,\n",
    "                                                                   test_size=0.3,\n",
    "                                                                   random_state=12345)\n",
    "\n",
    "print(\"Training data:\", len(features_train)/len(df), \"Test data:\", len(features_test)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test subsets for scaled data\n",
    "\n",
    "features_train_sc, features_test_sc, target_train_sc, target_test_sc = train_test_split(features_sc,\n",
    "                                                                   target_sc,\n",
    "                                                                   test_size=0.3,\n",
    "                                                                   random_state=12345)\n",
    "\n",
    "print(\"Training data:\", len(features_train_sc)/len(df_scaled_bin), \"Test data:\", len(features_test_sc)/len(df_scaled_bin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the class imbalance with value_counts() for main df data\n",
    "\n",
    "print(\"Counts\")\n",
    "print(\"Full data set:\", target.value_counts())\n",
    "print()\n",
    "print(\"Proportion\")\n",
    "print(\"Full data set:\", target.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Training data set:\", target_train.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test data set:\", target_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the class imbalance for scaled data\n",
    "\n",
    "print(\"Counts\")\n",
    "print(\"Full data set:\", target_sc.value_counts())\n",
    "print()\n",
    "print(\"Proportion\")\n",
    "print(\"Full data set:\", target_sc.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Training data set:\", target_train_sc.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test data set:\", target_test_sc.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to run F1 score and confusion matrix\n",
    "\n",
    "def eval_classifier(y_true, y_pred):\n",
    "    \n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(f'F1: {f1_score:.2f}')\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')\n",
    "    print('Confusion Matrix')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a kNN classifier to predict target on test data for k=1...10\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(\"Integer\", i)\n",
    "    neigh=sklearn.neighbors.KNeighborsClassifier(n_neighbors=i, metric='euclidean')\n",
    "    neigh.fit(features_train, target_train)\n",
    "    eval_classifier(target_test, neigh.predict(features_test))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a kNN classifier to predict target on scaled test data for k=1...10\n",
    "\n",
    "for i in range(1, 11):\n",
    "    print(\"Integer\", i)\n",
    "    neigh=sklearn.neighbors.KNeighborsClassifier(n_neighbors=i, metric='euclidean')\n",
    "    neigh.fit(features_train_sc, target_train_sc)\n",
    "    eval_classifier(target_test_sc, neigh.predict(features_test_sc))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating output of a random model\n",
    "\n",
    "def rnd_model_predict(P, size, seed=42):\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    return rng.binomial(n=1, p=P, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run probability and F1 score for random model\n",
    "\n",
    "for P in [0, df['insurance_benefits_received'].sum() / len(df), 0.5, 1]:\n",
    "\n",
    "    print(f'The probability: {P:.2f}')\n",
    "    y_pred_rnd = rnd_model_predict(P, len(df))\n",
    "        \n",
    "    eval_classifier(df['insurance_benefits_received'], y_pred_rnd)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Conclusions**\n",
    "The scaled data, k=1, produces the best results with F1 score of 0.97, which is a very strong result. Overall, the scaled data produces stronger results (F1 score range of 0.97-0.88 for k=1-10) than the original data (F1 score 0.61-0.00 for k=1-10). However, both data sets produced stronger results than the random model, where the F1 score did not surpass 0.20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Regression (with Linear Regression)\n",
    "\n",
    "With `insurance_benefits` as the target, evaluate what RMSE would be for a Linear Regression model.\n",
    "\n",
    "Build an implementation of LR. Check RMSE for both the original data and the scaled one. \n",
    "\n",
    "The following will guide the task:\n",
    "- $X$ — feature matrix, each row is a case, each column is a feature, the first column consists of unities\n",
    "- $y$ — target (a vector)\n",
    "- $\\hat{y}$ — estimated tagret (a vector)\n",
    "- $w$ — weight vector\n",
    "\n",
    "The task of linear regression in the language of matrices can be formulated as\n",
    "\n",
    "$$\n",
    "y = Xw\n",
    "$$\n",
    "\n",
    "The training objective is to find such $w$ that it would minimize the L2-distance (MSE) between $Xw$ and $y$:\n",
    "\n",
    "$$\n",
    "\\min_w d_2(Xw, y) \\quad \\text{or} \\quad \\min_w \\text{MSE}(Xw, y)\n",
    "$$\n",
    "\n",
    "The analytical solution for the above:\n",
    "\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "The formula above can be used to find the weights $w$ and the latter can be used to calculate predicted values:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X_{val}w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LR class\n",
    "\n",
    "class MyLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        self.weights = np.linalg.inv(np.dot(X2.T, X2)).dot(X2.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        y_pred = X2.dot(self.weights)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RMSE function\n",
    "\n",
    "def eval_regressor(y_true, y_pred):\n",
    "    \n",
    "    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    \n",
    "    r2_score = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))\n",
    "    print(f'R2: {r2_score:.2f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LR on original data and evaluate with RMSE\n",
    "\n",
    "X = df[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y = df['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr = MyLinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LR and evaluate RMSE with scaled data\n",
    "\n",
    "X_sc = df_scaled[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y_sc = df_scaled['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_sc_train, X_sc_test, y_sc_train, y_sc_test = train_test_split(X_sc, y_sc, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr_sc = MyLinearRegression()\n",
    "\n",
    "lr_sc.fit(X_sc_train, y_sc_train)\n",
    "print(lr_sc.weights)\n",
    "\n",
    "y_test_pred_sc = lr_sc.predict(X_sc_test)\n",
    "eval_regressor(y_sc_test, y_test_pred_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Conclusions**\n",
    "The RMSE for both the original and scaled data is the same at 0.34, and R2 is 0.66. Given insurance benefits range from 0 to 5, an RMSE is a modest result, suggesting the model is a modest fit for the data for both scaled and unscaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Obfuscating Data\n",
    "\n",
    "Obfuscate data by multiplying the numerical features (matrix $X$) by an invertible matrix $P$. \n",
    "\n",
    "$$\n",
    "X' = X \\times P\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info_column_list = ['gender', 'age', 'income', 'family_members']\n",
    "df_pn = df[personal_info_column_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pn.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random matrix P\n",
    "rng = np.random.default_rng(seed=42)\n",
    "P = rng.random(size=(X.shape[1], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to check invertibility by checking the determinant is not equal to 0\n",
    "\n",
    "def inv_P(matrix):\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        return \"not square\"\n",
    "    det = np.linalg.det(matrix)\n",
    "    return \"invertible\" if det != 0 else \"not invertible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function for P to confirm matrix P is invertible\n",
    "\n",
    "print(inv_P(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformed data matrix\n",
    "X_transformed = np.dot(X, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original and transformed matrices to check if ages or income apparent\n",
    "print(X[:5])\n",
    "\n",
    "print(X_transformed[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transformation, ages and income not decipherable. Next, check if possible to recover the original data from X' if P is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to recover the original data multiply the transformed data by the inverted P matrix\n",
    "\n",
    "# first create the inverted matrix for P\n",
    "P_inverse = np.linalg.inv(P)\n",
    "\n",
    "print(P_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the inverted P matrix with transformed data using the dot function\n",
    "X_recovered = np.dot(X_transformed, P_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if recovered and original matrix are the same\n",
    "print(\"Original data recovered accurately:\", np.allclose(X, X_recovered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original data, transformed data, and reversed (recovered) data\n",
    "# print sample from each data set\n",
    "\n",
    "print()\n",
    "print(\"Original X\")\n",
    "print(X[:5])\n",
    "print()\n",
    "print(\"Transformed X\")\n",
    "print(X_transformed[:5])\n",
    "print()\n",
    "print(\"Recovered X\")\n",
    "print(X_recovered[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Conclusions**\n",
    "\n",
    "The original and recovered data sets are similar but different by very small amounts. Recovering the data entails multiplying the transformed data by the inverted P matrix, both of which include rounded decimal numbers that will result in slightly different recovered data.\n",
    "\n",
    "In this section, a random, invertible matrix, P, was created and multiplied with the features data to create a new, obfuscated data set where data such as age and income was unrecognizable (e.g., if there was a data security breach). This data was then retransformed using an inverted version of the P matrix; this was done to determine if the original data could be accurately recovered, which would be required in an actual business application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof that Data Obfuscation Can Work with LR\n",
    "\n",
    "Analytical proof that the above obfuscation method does not affect linear regression predicted values.\n",
    "\n",
    "For this proof, I will first solve for the coefficient vector $w_p $ using matrix properties. I will then apply the simplified formula to the linear regression prediction formula $a = Xw $\n",
    "\n",
    "We can use the following properties of matrices to do this, which will be referenced in the proof.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>Distributivity</td><td>$A(B+C)=AB+AC$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Non-commutativity</td><td>$AB \\neq BA$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Associative property of multiplication</td><td>$(AB)C = A(BC)$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Multiplicative identity property</td><td>$IA = AI = A$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td></td><td>$A^{-1}A = AA^{-1} = I$\n",
    "</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>Reversivity of the transpose of a product of matrices,</td><td>$(AB)^T = B^TA^T$</td>\n",
    "</tr>    \n",
    "</table>\n",
    "\n",
    "\n",
    "**4.1 Solving for $w_p $**\n",
    "\n",
    "We solve for the coefficient vector $w_p$ of the obfuscation algorithm using matrix properties as follows. This will show how $w $ and $w_p $ are linked. The original formula:\n",
    "\n",
    "$w_P = [(XP)^T XP]^{-1} (XP)^T y$\n",
    "\n",
    "Separate the transpose matrices using reversivity of the transpose of a product of matrices property:\n",
    "\n",
    "$w_p = [(P^TX^T)XP]^{-1}P^TX^Ty $\n",
    "\n",
    "Distribute the inverse signs:\n",
    "\n",
    "$w_p = [(XP)^{-1}(P^TX^T)^{-1}]P^TX^Ty $\n",
    "\n",
    "$w_p = [P^{-1}X^{-1}X^{T-1}P^{T-1}]P^TX^Ty $\n",
    "\n",
    "Reformulate using the associative property of multiplication:\n",
    "\n",
    "$w_p = P^{-1}[X^{-1}(X^T)^{-1}](P^{T-1})P^TX^Ty $\n",
    "\n",
    "Replace $(P^{T-1})P^T $ with the identity matrix:\n",
    "\n",
    "$w_p = P^{-1}[X^{-1}(X^T)^{-1}]IX^Ty $\n",
    "\n",
    "Based on the multiplicative identity property, $IA = AI = A $ and we can remove the identity matrix:\n",
    "\n",
    "$w_p = P^{-1}[X^{-1}(X^T)^{-1}]X^Ty $\n",
    "\n",
    "Redistribute the inverse signs outside the brackets:\n",
    "\n",
    "$w_p = P^{-1}[(X^T)X]^{-1}X^Ty $\n",
    "\n",
    "Replace the ordinary least squares formula with $w $\n",
    "\n",
    "$w_p = P^{-1}w $\n",
    "\n",
    "\n",
    "**4.2 Consider affect of obfuscation on linear regression model**\n",
    "\n",
    "We next look at the predicted values for $a' $\n",
    "\n",
    "The linear regression model (predicted values) for $a $ is $a = Xw $\n",
    "\n",
    "When adding obfuscation, the model changes to account for the changed intercept and weights: $a' = XPw_P $\n",
    "\n",
    "Replace $w_p $ with the value from above:\n",
    "\n",
    "$a' = XPP^{-1}w $\n",
    "\n",
    "Replace $PP^{-1} with the identify matrix and then remove:\n",
    "\n",
    "$a' = XIw $\n",
    "\n",
    "$a' = Xw $\n",
    "\n",
    "Replace $Xw $ with $a $ from the linear regression model and we get:\n",
    "\n",
    "$a' = a $\n",
    "\n",
    "This shows that we get the same predictions using the original and obfuscated data.\n",
    "\n",
    "**Analytical Proof**\n",
    "Implications for the quality of the linear regression when measured by RMSE are as follows.  Based on the logic and algebra above, the coefficient vector of the obfuscated data (matrix multiplied by an invertible matrix $P $) is the same as the original coefficient vector multiplied by the inverted matrix P or $P^{-1}w $. By extension, the linear regression predictions for both the original data and obfuscated data will be the same, i.e., $a = a' $. This means that the RMSE will also be the same for the original and obfuscated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Test Linear Regression with Data Obfuscation\n",
    "\n",
    "Build a procedure or a class that runs Linear Regression optionally with the obfuscation. Run Linear Regression for the original data and the obfuscated one, compare the predicted values and the RMSE, $R^2$ metric values. \n",
    "\n",
    "**Procedure**\n",
    "\n",
    "- Create a square matrix $P$ of random numbers.\n",
    "- Check that it is invertible. If not, repeat the first point until we get an invertible matrix.\n",
    "- Create features and target from scaled data earlier in the project. Scaled data is used given it performed better than original non-scaled data.\n",
    "- Obfuscate features data and save as $XP$ matrix. \n",
    "- Create training and test data for scaled data, both regular scaled and obfuscated scaled data sets.\n",
    "- Run linear regression model on regular scaled data and obfuscated scaled data. Run accompanying RMSE and R2 for data and compare between obfuscated and unobfuscated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create square matrix P\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "P = rng.random(size=(X.shape[1], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sample to confirm created and check for invertibility\n",
    "print(P[:5])\n",
    "\n",
    "print(inv_P(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaled data set\n",
    "\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())\n",
    "\n",
    "df_scaled_lr = df.copy()\n",
    "df_scaled_lr.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())\n",
    "\n",
    "# create features and target from scaled data\n",
    "\n",
    "features_sc = df_scaled_lr[feature_names]\n",
    "target_sc = df_scaled_lr['insurance_benefits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create obfuscated data for features, XP\n",
    "\n",
    "XP = np.dot(features_sc, P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data for orig scaled data\n",
    "\n",
    "X_sc_train, X_sc_test, y_sc_train, y_sc_test = train_test_split(features_sc, \n",
    "                                                                target_sc, \n",
    "                                                                test_size=0.3, \n",
    "                                                                random_state=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test data for obf data - target is the same data as orig scaled data\n",
    "\n",
    "XP_train, XP_test, y_obf_train, y_obf_test = train_test_split(XP, \n",
    "                                                                target_sc, \n",
    "                                                                test_size=0.3, \n",
    "                                                                random_state=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on orig scaled data with eval metrics\n",
    "\n",
    "lr_sc = MyLinearRegression()\n",
    "\n",
    "lr_sc.fit(X_sc_train, y_sc_train)\n",
    "print(lr_sc.weights)\n",
    "\n",
    "y_test_pred_sc = lr_sc.predict(X_sc_test)\n",
    "eval_regressor(y_sc_test, y_test_pred_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on obf data with eval metrics\n",
    "\n",
    "lr_obf = MyLinearRegression()\n",
    "\n",
    "lr_obf.fit(XP_train, y_obf_train)\n",
    "print(lr_obf.weights)\n",
    "\n",
    "y_test_pred_obf = lr_obf.predict(XP_test)\n",
    "eval_regressor(y_obf_test, y_test_pred_obf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Conclusions**\n",
    "In this section, a Linear Regression model was tested with scaled data, comparing the results with the scaled data and with scaled data in which identifiable variables (i.e., features) were obfuscated. The RMSE and R2 results were the same for both data sets, demonstrating that obfuscated data can be used in place of unobfuscated data without compromising results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the Sure Tomorrow insurance company wanted to look at customer data to determine the likelihood customers would receive an insurance benefit. To do this, customers were compared to find similar customers, and regression models developed and adjusted to find a model that accurately predicts whether a given customer would likely receive an insurance benefit. Given the sensitivity of customer insurance data, customer data was obfuscated to better protect customer information, and model results compared with unobfuscated data to ensure continued model accuracy. The independent variables in this data set were gender, age, income level, and family size (number of additional family members other than primary beneficiary). The dependent variable was the number of insurance benefits received. \n",
    "\n",
    "The following was completed:\n",
    "- Libraries and data loaded, and data reviewed to ensure data types appropriate and no major data anomalies. Data types adjusted as appropriate.\n",
    "- Basic exploratory data analysis completed (graphs) to determine any obvious patterns.\n",
    "- A k nearest neighbors (kNN) algorithm was used to identify similar data points within the data set, i.e., for any given customer, $k $ customers most similar to the originally identified customer. \n",
    "- The kNN algorithm was tested to compare Euclidean and Manhattan distance metrics and the effect of scaling customer data (independent variables).\n",
    "- A k nearest neighbors classification algorithm was tested to predict whether customers would receive insurance benefits or not. For this model, the target was whether or not a customer received any benefits (not the number of benefits received). Original and scaled data were tested and compared with the model to determine whether scaled data resulted in a more accurate model. The number of comparable data points ($k $) were adjusted to determine the best fit. The kNN classification model was also compared to a dummy, random model. \n",
    "- A linear regression model was tested with original and scaled data to predict the number of insurance benefits received by a given customer. To do this, a class was developed that used a features matrix with a unities column, then finding the weight vector that best results in predicted number of benefits received. Predicted results and actual results were evaluated using RMSE and R2.\n",
    "- Data was masked by multiplying features data with an invertible matrix, P. The obfuscated data was then recovered by an inverted P matrix to confirm it was recoverable and comparable to the original data.\n",
    "- An analytical proof was developed to demonstrate that obfuscated data resulted in the same results when using a linear regression model.\n",
    "- Finally, a linear regression model was used with the obfuscated data and results compared to unobfuscated data. This step complements the analytical proof above and tests practically whether obfuscated data results in differences in the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following conclusions emerged:\n",
    "- Scaling data resulted in meaningful differences in identifying similar data points, as well as in more accurate data modeling with a classification model. Scaled data performed the same when using linear regression based on RMSE and R2 evaluation metrics.\n",
    "- Using Euclidean and Manhattan distance metrics resulted in only small differences in distances when identifying similar data points.\n",
    "- The kNN classification model had strong results with scaled data, with the best results for k=1 (F1 score = 0.97). The kNN model performed better overall than the random model for which the highest F1 score was 0.20 based on performance testing.\n",
    "- The kNN Linear Regression model had modest results (RMSE = 0.34, R2 = 0.66), with no differences in evaluation metrics results for scaled vs. unscaled data. \n",
    "- Obfuscated data masked identifiable data, such as income level or age, and was recoverable using an inverted version of the P matrix that was initially used to transform the data. Recovered data was slightly different than original data, but not significantly (i.e., differences were within tolerance values). \n",
    "- Using linear algebra and matrix properties, it was proved that obfuscated data will not affect linear regression model accuracy as compared to unobfuscated data.\n",
    "- The final linear regression model testing futher demonstrated that obfuscating data does not hinder the accuracy of the model. RMSE and R2 results were the same for both unobfuscated and obfuscated results (RMSE = 0.34, R2 = 0.66).\n",
    "\n",
    "Based on the full project, it is determined that a linear classification model will accurately predict whether a given insurance customer will receive any insurance benefits, particularly if using scaled features data (independent variables data). Furthermore, a regression model will result in being able to accurately predict the number of benefits a customer will receive based on independent variables (gender, age, income, number of family members). Using obfuscated data protects clients' data without compromising model results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
